{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Previous BERT model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "from transformers import AutoTokenizer,AutoModel,AutoConfig\n",
    "from transformers.modeling_outputs import TokenClassifierOutput\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class HangmanNet(nn.Module):\n",
    "  def __init__(self,checkpoint, vocab_size = 26, hidden_ffn_size = 410, unfreeze_layers = 0): \n",
    "    super(HangmanNet,self).__init__() \n",
    "    self.num_labels = vocab_size \n",
    "\n",
    "    #Load Model with given checkpoint and extract its body\n",
    "    self.model = AutoModel.from_pretrained(checkpoint,config=AutoConfig.from_pretrained(checkpoint, output_attentions=True,output_hidden_states=True))\n",
    "    \n",
    "    # Freeze all layers in the BERT model\n",
    "    for param in self.soruce_modelparameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    # Unfreeze the last `unfreeze_layers` layers\n",
    "    if unfreeze_layers > 0:\n",
    "        for layer in self.model.encoder.layer[-unfreeze_layers:]:\n",
    "            for param in layer.parameters():\n",
    "                param.requires_grad = True\n",
    "        \n",
    "\n",
    "    self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    self.classifier = nn.Sequential(\n",
    "        nn.Linear(768 + vocab_size, hidden_ffn_size),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(hidden_ffn_size, 26)\n",
    "    )\n",
    "    \n",
    "    # self.classifier = nn.Linear(768 + vocab_size,vocab_size) # load and initialize weights\n",
    "  \n",
    "  def forward(self, input_ids=None, attention_mask=None, labels=None, prev_guess=None,\n",
    "              token_type_ids=None):\n",
    "      outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "      \n",
    "      sequence_output = outputs.last_hidden_state  # (batch_size, sequence_length, hidden_size)\n",
    "      sequence_output = self.dropout(sequence_output)\n",
    "\n",
    "      # Concatenate the previous guesses to the sequence_output\n",
    "      # (batch_size, sequence_length, hidden_size + vocab_size)\n",
    "      sequence_output = torch.cat((sequence_output, prev_guess.unsqueeze(1).repeat(1, sequence_output.shape[1], 1)), dim=2)\n",
    "\n",
    "      logits = self.classifier(sequence_output)  # (batch_size, sequence_length, num_labels)\n",
    "\n",
    "      \n",
    "      # Mask the logits to zero out probabilities of previously guessed characters by considering the one-hot encoding in prev guesses\n",
    "      logits[prev_guess.unsqueeze(1).repeat(1, sequence_output.shape[1], 1) == 1] = -float(\"inf\")\n",
    "\n",
    "      loss = None\n",
    "      if labels is not None:\n",
    "          loss_fct = nn.CrossEntropyLoss()\n",
    "\n",
    "          # NOTE: I already has the labels in active logits representation\n",
    "          active_logits = logits.view(-1, self.num_labels)\n",
    "\n",
    "        #   active_labels = torch.where(active_loss, labels.view(-1), torch.tensor(loss_fct.ignore_index).type_as(labels))\n",
    "          loss = loss_fct(active_logits, labels.view(-1))\n",
    "        \n",
    "      return TokenClassifierOutput(logits=logits, loss=loss, hidden_states=outputs.hidden_states,attentions=outputs.attentions)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InferenceTokenizer:\n",
    "    def __init__(self, tokenizer, max_length=42):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.char_to_index = {chr(i + ord('a')): i for i in range(26)}\n",
    "\n",
    "    def __call__(self, masked_word, prev_guesses):\n",
    "\n",
    "        prev_guesses_one_hot = torch.zeros(26, dtype=torch.int64)\n",
    "\n",
    "        for char in prev_guesses:\n",
    "            prev_guesses_one_hot[self.char_to_index[char]] = 1\n",
    "\n",
    "\n",
    "        # NOTE: The labels are not require during inference\n",
    "        # labels_word = examples['labels']\n",
    "\n",
    "        # Replace the underscore in masked_word with the special [MASK] token\n",
    "        masked_word = masked_word.replace('_', '[MASK]')\n",
    "\n",
    "        # Tokenize the masked_words\n",
    "        batch = self.tokenizer(masked_word, truncation=True, padding='max_length', return_tensors=\"pt\", max_length=self.max_length)\n",
    "\n",
    "        batch['prev_guess'] = prev_guesses_one_hot.unsqueeze(0)\n",
    "\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_ p p _ e\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101,  103, 1052, 1052,  103, 1041,  102,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]]), 'prev_guess': tensor([[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0,\n",
       "         0, 0]])}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NOTE: Example of tokenization\n",
    "# word input example: \"_ p p _ e \"\n",
    "word = [\"_\", \"p\", \"p\", \"_\", \"e\" ]\n",
    "\n",
    "word = \" \".join(word)\n",
    "print(word)\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Initialize Custom InferenceTokenizer\n",
    "tokenizer = InferenceTokenizer(tokenizer, max_length=32)\n",
    "\n",
    "\n",
    "tokenized_word = tokenizer(word, prev_guesses=['p','e', 'u', 'o', 't'])\n",
    "tokenized_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[PAD]'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "tokenizer.unk_token\n",
    "tokenizer.mask_token\n",
    "tokenizer.pad_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False, False,  True,  True],\n",
       "        [False,  True, False,  True]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "input_ids_example = torch.tensor([[101, 108, 103, 103], [101, 103, 102, 103]])\n",
    "\n",
    "# mask_idxs = torch.where(input_ids_example == 103)\n",
    "mask = (input_ids_example == 103)\n",
    "mask\n",
    "\n",
    "# per row chose one true randomly and set the rest to false\n",
    "mask = torch.zeros_like(input_ids_example, dtype=torch.bool)\n",
    "mask[torch.arange(input_ids_example.size(0)), torch.randint(0, input_ids_example.size(1), (input_ids_example.size(0),))]\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 2.1635, -0.5571, -0.5617, -1.3012, -0.2051,  0.5336],\n",
       "         [ 0.4887, -1.3061,  0.3810, -0.1133, -0.7405, -1.0193],\n",
       "         [ 0.5223,  0.1506, -0.9304,  0.3422,  1.9073, -1.5074],\n",
       "         [-1.1713,  0.3046, -1.4073,  0.2468,  1.5389,  0.1109]],\n",
       "\n",
       "        [[-0.0305,  0.8902,  0.1471, -0.7971, -0.9272, -0.5414],\n",
       "         [-0.4392,  1.1099,  0.0776,  0.4063, -0.1076,  0.6352],\n",
       "         [-1.1726,  0.1578,  1.1572, -0.3341,  1.0961,  0.0611],\n",
       "         [ 0.1343,  1.5638, -0.7380,  0.9475,  1.0093,  0.8183]]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_outputs = torch.randn(2, 4, 6)\n",
    "encoder_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Mask:\n",
      " tensor([[False, False,  True,  True],\n",
      "        [False,  True, False,  True]])\n",
      "Final Mask with One Random True per Row:\n",
      " tensor([[False, False,  True, False],\n",
      "        [False, False, False,  True]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 6])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Input tensor and mask\n",
    "input_ids_example = torch.tensor([[101, 108, 103, 103], [101, 103, 102, 103]])\n",
    "mask = (input_ids_example == 103)\n",
    "\n",
    "# Create random values for tie-breaking\n",
    "random_values = torch.rand_like(input_ids_example.float())\n",
    "\n",
    "# Set random values only where the mask is True\n",
    "random_values[~mask] = float('-inf')  # Set irrelevant positions to -inf\n",
    "\n",
    "# Find the index of the maximum random value per row\n",
    "_, selected_indices = random_values.max(dim=1)\n",
    "\n",
    "# Create the final mask\n",
    "final_mask = torch.zeros_like(mask, dtype=torch.bool)\n",
    "final_mask[torch.arange(mask.size(0)), selected_indices] = True\n",
    "\n",
    "# Display results\n",
    "print(\"Original Mask:\\n\", mask)\n",
    "print(\"Final Mask with One Random True per Row:\\n\", final_mask)\n",
    "\n",
    "encoder_outputs[final_mask].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0, 1], [2, 3])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Given tuple of indices\n",
    "row_indices = torch.tensor([0, 0, 1, 1])\n",
    "col_indices = torch.tensor([2, 3, 1, 3])\n",
    "\n",
    "# Unique column indices\n",
    "unique_cols = torch.unique(col_indices)\n",
    "\n",
    "# Storage for the selected indices\n",
    "selected_row_indices = []\n",
    "selected_col_indices = []\n",
    "\n",
    "num_rows = 2\n",
    "for row in range(num_rows):\n",
    "    # Get the row indices corresponding to this row\n",
    "    mask = row_indices == row\n",
    "    available_cols = col_indices[mask]\n",
    "    \n",
    "    # Randomly pick one column index\n",
    "    selected_col = available_cols[torch.randint(len(available_cols), (1,))]\n",
    "    \n",
    "    # Store the selection\n",
    "    selected_row_indices.append(row)\n",
    "    selected_col_indices.append(selected_col.item())\n",
    "\n",
    "selected_row_indices, selected_col_indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MASK ID: 103, PAD ID: 0, CLS ID: 101, UNK ID: 100\n"
     ]
    }
   ],
   "source": [
    "# Get the IDs for MASK and PAD tokens\n",
    "mask_id = tokenizer.mask_token_id\n",
    "pad_id = tokenizer.pad_token_id\n",
    "cls_id = tokenizer.cls_token_id\n",
    "unk_id = tokenizer.unk_token_id\n",
    "\n",
    "print(f\"MASK ID: {mask_id}, PAD ID: {pad_id}, CLS ID: {cls_id}, UNK ID: {unk_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: Function to encode with BERT in my environment\n",
    "\n",
    "def bert_tokenizer_encode_word(masked_word, tokenizer, max_length=32):\n",
    "    masked_word = \" \".join(masked_word)\n",
    "\n",
    "    # Initialize the tokenizer\n",
    "    tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "    masked_word = masked_word.replace('_', '[MASK]')\n",
    "\n",
    "    batch_info = tokenizer(masked_word, truncation=True, padding='max_length', return_tensors=\"np\", max_length=max_length)\n",
    "\n",
    "    return word, batch_info[\"input_ids\"].astype(np.int32)\n",
    "\n",
    "word = [\"_\", \"p\", \"p\", \"_\", \"e\" ]\n",
    "_, input_ids = bert_tokenizer_encode_word(word, tokenizer, max_length=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 101,  103, 1052, 1052,  103, 1041,  102,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
       "      dtype=int32)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101,  103, 1052, 1052,  103, 1041,  102,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'prev_guess': tensor([[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0,\n",
       "         0, 0]])}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NOTE: Example of tokenization\n",
    "# word input example: \"_ p p _ e \"\n",
    "word = \"_ p p _ e \"\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Initialize Custom InferenceTokenizer\n",
    "tokenizer = InferenceTokenizer(tokenizer, max_length=42)\n",
    "\n",
    "\n",
    "tokenized_word = tokenizer(word, prev_guesses=['p','e', 'u', 'o', 't'])\n",
    "tokenized_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zosov/anaconda3/envs/character-bert/lib/python3.10/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_64667/3321725431.py:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path)\n"
     ]
    }
   ],
   "source": [
    "# Load the model with the base bert model\n",
    "model = HangmanNet(checkpoint=\"bert-base-uncased\", vocab_size = 26, unfreeze_layers = 2)\n",
    "\n",
    "# Load my trained model an perform an inference\n",
    "# NOTE: Change the path accordingly if you are trying in other environment\n",
    "checkpoint_path = \"models/pretrained_bert/model_epoch_45.pth\"\n",
    "checkpoint = torch.load(checkpoint_path)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "soruce_model = checkpoint['model_state_dict']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HangmanNet(\n",
       "  (model): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=794, out_features=410, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=410, out_features=26, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils_model import CustomBERT\n",
    "# Initialize your CustomBERT model\n",
    "custom_bert = CustomBERT(\n",
    "    vocab_size=30,  # Or the appropriate vocab size\n",
    "    hidden_size=768,\n",
    "    num_hidden_layers=4,\n",
    "    num_attention_heads=4,\n",
    "    max_position_embeddings=512,\n",
    "    intermediate_size=3072,\n",
    "    dqn_head=True\n",
    ")\n",
    "\n",
    "# Transfer the weights\n",
    "# custom_bert = transfer_weights(hangman_state_dict, custom_bert)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Compare weights\n",
    "\n",
    "# print(torch.allclose(model.model.embeddings.word_embeddings.weight, custom_bert.embeddings.word_embeddings.weight, atol=1e-6))\n",
    "# print(torch.allclose(model.model.embeddings.position_embeddings.weight, custom_bert.embeddings.position_embeddings.weight, atol=1e-6))\n",
    "# print(torch.allclose(model.model.embeddings.token_type_embeddings.weight, custom_bert.embeddings.token_type_embeddings.weight, atol=1e-6))\n",
    "# print(torch.allclose(model.model.embeddings.LayerNorm.weight, custom_bert.embeddings.LayerNorm.weight, atol=1e-6))\n",
    "# print(torch.allclose(model.model.embeddings.LayerNorm.bias, custom_bert.embeddings.LayerNorm.bias, atol=1e-6))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save the updated custom bert model\n",
    "# torch.save(custom_bert.state_dict(), \"models/pretrained_bert_epoch_45.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zosov/anaconda3/envs/character-bert/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CustomBERT number of parameters:  4478618\n"
     ]
    }
   ],
   "source": [
    "from utils_model import CustomBERT\n",
    "\n",
    "# Load the updated custom bert model\n",
    "custom_bert = CustomBERT(\n",
    "    vocab_size=30,  # Or the appropriate vocab size\n",
    "    hidden_size=128,\n",
    "    num_hidden_layers=2,\n",
    "    num_attention_heads=2,\n",
    "    max_position_embeddings=512,\n",
    "    intermediate_size=512,\n",
    "    dqn_head=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CustomBERT(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 128, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 128)\n",
       "      (token_type_embeddings): Embedding(2, 128)\n",
       "      (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-1): 2 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (key): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (value): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (head): DQNHead(\n",
       "    (classifier): Sequential(\n",
       "      (0): Linear(in_features=154, out_features=512, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=512, out_features=26, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zosov/anaconda3/envs/character-bert/lib/python3.10/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 128, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 128)\n",
       "    (token_type_embeddings): Embedding(2, 128)\n",
       "    (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-1): 2 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSdpaSelfAttention(\n",
       "            (query): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (key): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (value): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=128, out_features=512, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=512, out_features=128, bias=True)\n",
       "          (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer,AutoModel,AutoConfig\n",
    "\n",
    "# Load the model prajjwal1/bert-tiny\n",
    "checkpoint = \"prajjwal1/bert-tiny\"\n",
    "pretrained_bert = AutoModel.from_pretrained(checkpoint,config=AutoConfig.from_pretrained(checkpoint, output_attentions=True,output_hidden_states=True))\n",
    "pretrained_bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights successfully copied from source to target.\n",
      "  embeddings.word_embeddings.weight\n",
      "  embeddings.position_embeddings.weight\n",
      "  embeddings.token_type_embeddings.weight\n",
      "  embeddings.LayerNorm.weight\n",
      "  embeddings.LayerNorm.bias\n",
      "  encoder.layer.0.attention.self.query.weight\n",
      "  encoder.layer.0.attention.self.query.bias\n",
      "  encoder.layer.0.attention.self.key.weight\n",
      "  encoder.layer.0.attention.self.key.bias\n",
      "  encoder.layer.0.attention.self.value.weight\n",
      "  encoder.layer.0.attention.self.value.bias\n",
      "  encoder.layer.0.attention.output.dense.weight\n",
      "  encoder.layer.0.attention.output.dense.bias\n",
      "  encoder.layer.0.attention.output.LayerNorm.weight\n",
      "  encoder.layer.0.attention.output.LayerNorm.bias\n",
      "  encoder.layer.0.intermediate.dense.weight\n",
      "  encoder.layer.0.intermediate.dense.bias\n",
      "  encoder.layer.0.output.dense.weight\n",
      "  encoder.layer.0.output.dense.bias\n",
      "  encoder.layer.0.output.LayerNorm.weight\n",
      "  encoder.layer.0.output.LayerNorm.bias\n",
      "  encoder.layer.1.attention.self.query.weight\n",
      "  encoder.layer.1.attention.self.query.bias\n",
      "  encoder.layer.1.attention.self.key.weight\n",
      "  encoder.layer.1.attention.self.key.bias\n",
      "  encoder.layer.1.attention.self.value.weight\n",
      "  encoder.layer.1.attention.self.value.bias\n",
      "  encoder.layer.1.attention.output.dense.weight\n",
      "  encoder.layer.1.attention.output.dense.bias\n",
      "  encoder.layer.1.attention.output.LayerNorm.weight\n",
      "  encoder.layer.1.attention.output.LayerNorm.bias\n",
      "  encoder.layer.1.intermediate.dense.weight\n",
      "  encoder.layer.1.intermediate.dense.bias\n",
      "  encoder.layer.1.output.dense.weight\n",
      "  encoder.layer.1.output.dense.bias\n",
      "  encoder.layer.1.output.LayerNorm.weight\n",
      "  encoder.layer.1.output.LayerNorm.bias\n",
      "  pooler.dense.weight\n",
      "  pooler.dense.bias\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def copy_bert_weights(source_model, target_model):\n",
    "    \"\"\"\n",
    "    Copy the weights of the `BertModel` component from `source_model` to `target_model`.\n",
    "    \n",
    "    Parameters:\n",
    "        source_model: The source model (e.g., `BertModel` instance).\n",
    "        target_model: The target model (e.g., `CustomBERT` instance).\n",
    "    \"\"\"\n",
    "    # Extract the state_dict from the source and target models\n",
    "    source_state_dict = source_model.state_dict()\n",
    "    target_state_dict = target_model.bert.state_dict()  # Assuming target model has a .bert attribute\n",
    "    \n",
    "    # Filter out weights that are specific to the source or target\n",
    "    filtered_source_state_dict = {\n",
    "        k: v for k, v in source_state_dict.items() if k in target_state_dict\n",
    "    }\n",
    "    \n",
    "    # Update the target model with the source weights\n",
    "    target_state_dict.update(filtered_source_state_dict)\n",
    "    \n",
    "    # Load the updated state_dict back into the target model\n",
    "    target_model.bert.load_state_dict(target_state_dict)\n",
    "    print(\"Weights successfully copied from source to target.\")\n",
    "    for k, v in filtered_source_state_dict.items():\n",
    "        print(f\"  {k}\")\n",
    "\n",
    "# Example usage\n",
    "# Assuming `source_bert_model` is an instance of `BertModel` and\n",
    "# `custom_bert_model` is an instance of `CustomBERT`\n",
    "copy_bert_weights(pretrained_bert, custom_bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-4.1018e-03, -3.0695e-02, -3.5295e-03,  ...,  1.8925e-02,\n",
       "          3.7396e-03, -2.9233e-03],\n",
       "        [-4.2748e-04, -3.6929e-02, -1.7168e-02,  ...,  2.9314e-02,\n",
       "         -1.0398e-02,  2.6772e-02],\n",
       "        [ 5.9418e-03,  4.2119e-03, -1.9566e-02,  ...,  1.6799e-02,\n",
       "         -2.7802e-02, -6.9017e-03],\n",
       "        ...,\n",
       "        [ 3.5573e-02, -1.5891e-02,  4.9951e-03,  ...,  5.4071e-03,\n",
       "         -1.1270e-02, -6.9528e-05],\n",
       "        [-8.7018e-03, -2.2516e-02,  3.1993e-03,  ...,  2.7591e-02,\n",
       "         -1.9554e-02,  2.4023e-03],\n",
       "        [-7.8904e-02, -7.5407e-02, -4.6660e-03,  ..., -5.3340e-03,\n",
       "         -4.4993e-02,  5.9842e-02]], requires_grad=True)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_bert.bert.embeddings.word_embeddings.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(custom_bert.state_dict(), \"models/bert_tiny.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CustomBERT number of parameters:  4460156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_115211/3243659273.py:15: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  custom_bert.load_state_dict(torch.load(\"models/bert_tiny.pth\"))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils_model import CustomBERT\n",
    "import torch\n",
    "# Load from the saved model\n",
    "custom_bert = CustomBERT(\n",
    "    vocab_size=30,  # Or the appropriate vocab size\n",
    "    hidden_size=128,\n",
    "    num_hidden_layers=2,\n",
    "    num_attention_heads=2,\n",
    "    max_position_embeddings=512,\n",
    "    intermediate_size=512,\n",
    "    dqn_head=True\n",
    ")\n",
    "\n",
    "# Load the updated custom bert model\n",
    "custom_bert.load_state_dict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Freeze all parameters\n",
    "# for param in custom_bert.parameters():\n",
    "#     param.requires_grad = False\n",
    "\n",
    "# # Unfreeze the last layer of the encoder\n",
    "# # for param in custom_bert.encoder.layer[-1].parameters():\n",
    "# #     param.requires_grad = True\n",
    "\n",
    "# # Unfreeze the head\n",
    "# for param in custom_bert.head.parameters():\n",
    "#     param.requires_grad = True\n",
    "\n",
    "# # Count trainable parameters\n",
    "# def count_trainable_parameters(model):\n",
    "#     return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "# trainable_params = count_trainable_parameters(custom_bert)\n",
    "# print(f\"Number of trainable parameters: {trainable_params}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class LoRALinear(nn.Module):\n",
    "    def __init__(self, in_features, out_features, r=8, alpha=16):\n",
    "        \"\"\"\n",
    "        LoRA linear layer.\n",
    "        :param in_features: Input size of the linear layer.\n",
    "        :param out_features: Output size of the linear layer.\n",
    "        :param r: Rank of the low-rank adaptation.\n",
    "        :param alpha: Scaling factor for the low-rank matrices.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.r = r\n",
    "        self.alpha = alpha\n",
    "        self.scaling = alpha / r\n",
    "        self.weight = nn.Parameter(torch.zeros(out_features, in_features))\n",
    "        self.lora_A = nn.Parameter(torch.randn(r, in_features))\n",
    "        self.lora_B = nn.Parameter(torch.randn(out_features, r))\n",
    "        self.bias = nn.Parameter(torch.zeros(out_features))\n",
    "\n",
    "        nn.init.zeros_(self.weight)\n",
    "        nn.init.kaiming_uniform_(self.lora_A, a=math.sqrt(5))\n",
    "        nn.init.zeros_(self.lora_B)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return (\n",
    "            torch.nn.functional.linear(x, self.weight, self.bias)\n",
    "            + self.scaling * torch.nn.functional.linear(\n",
    "                torch.nn.functional.linear(x, self.lora_A), self.lora_B\n",
    "            )\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_lora(bert_model, last_layers = 1, r=4, alpha=16):\n",
    "    for i in range(-last_layers, 0):  # Replace the last two layers\n",
    "        layer = bert_model.encoder.layer[i]\n",
    "        \n",
    "        # Replace attention linear layers with LoRA versions\n",
    "        layer.attention.self.query = LoRALinear(\n",
    "            in_features=layer.attention.self.query.in_features,\n",
    "            out_features=layer.attention.self.query.out_features,\n",
    "            r=r,\n",
    "            alpha=alpha\n",
    "        )\n",
    "        layer.attention.self.key = LoRALinear(\n",
    "            in_features=layer.attention.self.key.in_features,\n",
    "            out_features=layer.attention.self.key.out_features,\n",
    "            r=r,\n",
    "            alpha=alpha\n",
    "        )\n",
    "        layer.attention.self.value = LoRALinear(\n",
    "            in_features=layer.attention.self.value.in_features,\n",
    "            out_features=layer.attention.self.value.out_features,\n",
    "            r=r,\n",
    "            alpha=alpha\n",
    "        )\n",
    "        \n",
    "        # Replace intermediate and output dense layers with LoRA versions\n",
    "        # layer.intermediate.dense = LoRALinear(\n",
    "        #     in_features=layer.intermediate.dense.in_features,\n",
    "        #     out_features=layer.intermediate.dense.out_features,\n",
    "        #     r=r,\n",
    "        #     alpha=alpha\n",
    "        # )\n",
    "        # layer.output.dense = LoRALinear(\n",
    "        #     in_features=layer.output.dense.in_features,\n",
    "        #     out_features=layer.output.dense.out_features,\n",
    "        #     r=r,\n",
    "        #     alpha=alpha\n",
    "        # )\n",
    "\n",
    "    return bert_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freezing_layers_and_LoRA(custom_bert):\n",
    "    # Freeze all parameters\n",
    "    for param in custom_bert.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    # Unfreeze the last layer of the encoder\n",
    "    # for param in custom_bert.encoder.layer[-1].parameters():\n",
    "    #     param.requires_grad = True\n",
    "\n",
    "    # Unfreeze the head\n",
    "    for param in custom_bert.head.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "    # Apply LoRA to the last two layers\n",
    "    custom_bert = apply_lora(custom_bert, r=4, alpha=16)\n",
    "\n",
    "\n",
    "    # Count trainable parameters\n",
    "    def count_trainable_parameters(model):\n",
    "        return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "    trainable_params = count_trainable_parameters(custom_bert)\n",
    "    print(f\"Number of trainable parameters: {trainable_params}\")\n",
    "\n",
    "    return custom_bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters: 2126844\n"
     ]
    }
   ],
   "source": [
    "custom_bert = freezing_layers_and_LoRA(custom_bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the following model prajjwal1/bert-tiny\n",
    "\n",
    "# Load the model with the base bert model\n",
    "model = HangmanNet(checkpoint=\"prajjwal1/bert-tiny\", vocab_size = 26, unfreeze_layers = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'BertLayer' object has no attribute 'attention.self.query.weight'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 17\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m12\u001b[39m):\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention.self.query.weight\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention.self.query.bias\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     10\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention.self.key.weight\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention.self.key.bias\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     11\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention.self.value.weight\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention.self.value.bias\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     15\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput.dense.weight\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput.dense.bias\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     16\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput.LayerNorm.weight\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput.LayerNorm.bias\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[0;32m---> 17\u001b[0m         \u001b[38;5;28mprint\u001b[39m(torch\u001b[38;5;241m.\u001b[39mallclose(\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getattr__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m, custom_bert\u001b[38;5;241m.\u001b[39mencoder\u001b[38;5;241m.\u001b[39mlayer[i]\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattr__\u001b[39m(key), atol\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-6\u001b[39m))\n",
      "File \u001b[0;32m~/anaconda3/envs/character-bert/lib/python3.10/site-packages/torch/nn/modules/module.py:1931\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1929\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1930\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1931\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m   1932\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1933\u001b[0m )\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'BertLayer' object has no attribute 'attention.self.query.weight'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# # Check if the encoder layers are approximately equal\n",
    "# for i in range(12):\n",
    "#     for key in ['attention.self.query.weight', 'attention.self.query.bias',\n",
    "#                 'attention.self.key.weight', 'attention.self.key.bias',\n",
    "#                 'attention.self.value.weight', 'attention.self.value.bias',\n",
    "#                 'attention.output.dense.weight', 'attention.output.dense.bias',\n",
    "#                 'attention.output.LayerNorm.weight', 'attention.output.LayerNorm.bias',\n",
    "#                 'intermediate.dense.weight', 'intermediate.dense.bias',\n",
    "#                 'output.dense.weight', 'output.dense.bias',\n",
    "#                 'output.LayerNorm.weight', 'output.LayerNorm.bias']:\n",
    "#         print(torch.allclose(model.model.encoder.layer[i].__getattr__(key), custom_bert.encoder.layer[i].__getattr__(key), atol=1e-6))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertLayer(\n",
       "  (attention): BertAttention(\n",
       "    (self): BertSdpaSelfAttention(\n",
       "      (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (output): BertSelfOutput(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (intermediate): BertIntermediate(\n",
       "    (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "    (intermediate_act_fn): GELUActivation()\n",
       "  )\n",
       "  (output): BertOutput(\n",
       "    (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.encoder.layer[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleList(\n",
       "  (0-11): 12 x BertLayer(\n",
       "    (attention): BertAttention(\n",
       "      (self): BertSelfAttention(\n",
       "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (output): BertSelfOutput(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (intermediate): BertIntermediate(\n",
       "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "      (intermediate_act_fn): GELUActivation()\n",
       "    )\n",
       "    (output): BertOutput(\n",
       "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_bert.encoder.layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.0102, -0.0615, -0.0265,  ..., -0.0199, -0.0372, -0.0098],\n",
       "        [-0.0117, -0.0600, -0.0323,  ..., -0.0168, -0.0401, -0.0107],\n",
       "        [-0.0198, -0.0627, -0.0326,  ..., -0.0165, -0.0420, -0.0032],\n",
       "        ...,\n",
       "        [-0.0218, -0.0556, -0.0135,  ..., -0.0043, -0.0151, -0.0249],\n",
       "        [-0.0462, -0.0565, -0.0019,  ...,  0.0157, -0.0139, -0.0095],\n",
       "        [ 0.0015, -0.0821, -0.0160,  ..., -0.0081, -0.0475,  0.0753]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "character-bert",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
