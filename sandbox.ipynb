{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pretrained Char Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: Is there a embedding dataset of only words in the web?? Yes\n",
    "\n",
    "# Load a .txt file separated by spaces where the first colulms is the character and the other columns corresponds\n",
    "# to the word embedding\n",
    "\n",
    "# Load the dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Load char embeddings\n",
    "embeddings_path = 'glove.840B.300d-char.txt'\n",
    "\n",
    "print('Processing pretrained character embeds...')\n",
    "char_embeddings = {}\n",
    "with open(embeddings_path, 'r') as f:\n",
    "    for line in f:\n",
    "        line_split = line.strip().split(\" \")\n",
    "        vec = torch.tensor(np.array(line_split[1:], dtype=float))\n",
    "        char = line_split[0]\n",
    "        char_embeddings[char] = vec\n",
    "\n",
    "# For mask use underscore\n",
    "token_mask = char_embeddings[\"_\"]\n",
    "\n",
    "# For padding use point\n",
    "token_pad = char_embeddings[\".\"]\n",
    "\n",
    "# For CLS used to return an cumulated information for the whole word use #\n",
    "token_cls = char_embeddings[\"#\"]\n",
    "\n",
    "# Filter Character Embeddings for char from a-z and three special tokens\n",
    "char_embeddings = {char: vec for char, vec in char_embeddings.items() if char in \"abcdefghijklmnopqrstuvwxyz\"}\n",
    "# Assing char indeces from 0 to 25 to the letters and 26 to 28 to the special tokens\n",
    "char_indices = {char: i for i, char in enumerate(char_embeddings.keys())}\n",
    "\n",
    "char_embeddings[\"_\"] = token_mask\n",
    "char_embeddings[\".\"] = token_pad\n",
    "char_embeddings[\"#\"] = token_cls\n",
    "\n",
    "char_indices[\"_\"] = 26\n",
    "char_indices[\".\"] = 27\n",
    "char_indices[\"#\"] = 28\n",
    "\n",
    "# Create an embedding matrix E\n",
    "embedding_matrix = torch.zeros((len(char_embeddings), 300))\n",
    "#embedding_matrix = np.random.uniform(-1, 1, (len(chars), 300))\n",
    "for char, i in char_indices.items():\n",
    "    #print (\"{}, {}\".format(char, i))\n",
    "    embedding_vector = char_embeddings.get(char)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import Module\n",
    "from transformers import BertConfig, BertModel, BertTokenizer\n",
    "from transformers.models.bert.modeling_bert import BertEmbeddings, BertEncoder, BertPooler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Custom Character-Level Tokenizer\n",
    "class CharTokenizer:\n",
    "    def __init__(self):\n",
    "        self.vocab = {char: idx for idx, char in enumerate(\"abcdefghijklmnopqrstuvwxyz\", start=1)}\n",
    "        self.vocab[\"[PAD]\"] = 0\n",
    "        self.vocab[\"[UNK]\"] = len(self.vocab)\n",
    "        self.vocab[\"[CLS]\"] = len(self.vocab)\n",
    "        self.vocab[\"[MASK]\"] = len(self.vocab)\n",
    "        self.inv_vocab = {v: k for k, v in self.vocab.items()}\n",
    "        print(\"Vocab size: \", len(self.vocab))\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        return list(text.lower())\n",
    "    \n",
    "    def get_ids(self, tokens):\n",
    "        return [self.vocab.get(t, self.vocab[\"[UNK]\"]) for t in tokens]\n",
    "\n",
    "    def encode(self, text, max_length):\n",
    "        tokens = self.tokenize(text)\n",
    "        token_ids = self.get_ids(tokens)\n",
    "        return [self.vocab[\"[CLS]\"]] + token_ids[:max_length] + [self.vocab[\"[PAD]\"]] * (max_length - len(token_ids) - 1)\n",
    "    \n",
    "    def decode(self, token_ids):\n",
    "        return ''.join([self.inv_vocab.get(t, \"[UNK]\") for t in token_ids if t != 0])\n",
    "    \n",
    "# Custom Dataset\n",
    "class WordDataset(Dataset):\n",
    "    def __init__(self, words, tokenizer, max_length):\n",
    "        self.words = words\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.words)\n",
    "    \n",
    "    def mlm_masking_word(self, sentence):\n",
    "        # Tokenize the entire sentence\n",
    "        # tokenized = self.tokenizer(sentence, return_tensors=\"pt\", add_special_tokens=False)\n",
    "        # tokens = tokenized[\"input_ids\"].squeeze(0)  # Shape: (seq_len,)\n",
    "\n",
    "        tokens = self.tokenizer.tokenize(sentence)\n",
    "        token_ids = torch.tensor(self.tokenizer.get_ids(tokens))\n",
    "\n",
    "        # Generate random probabilities for each token\n",
    "        probs = torch.rand(token_ids.shape)\n",
    "\n",
    "        # 15% of the tokens will be considered for masking\n",
    "        mask_prob = probs < 0.15\n",
    "        # print(\"0.15 Masked:\",  mask_prob)\n",
    "\n",
    "        # Initialize labels (original tokens for masked positions, 0 otherwise)\n",
    "        labels = torch.where(mask_prob, token_ids, torch.zeros_like(token_ids))\n",
    "        # print(\"labels:\", labels)\n",
    "\n",
    "        # 80% of masked tokens will be replaced with [MASK]\n",
    "        mask_replace_prob = torch.rand(token_ids.shape)\n",
    "        masked_tokens = torch.where(\n",
    "            mask_prob & (mask_replace_prob < 0.8), \n",
    "            torch.tensor(self.tokenizer.vocab['[MASK]']), \n",
    "            token_ids\n",
    "        )\n",
    "        # print(\"80% from masked: \", mask_prob & (mask_replace_prob < 0.8))\n",
    "        # print(masked_tokens)\n",
    "\n",
    "        # 10% of masked tokens will be replaced with random tokens\n",
    "        random_replace_prob = torch.rand(token_ids.shape)\n",
    "        random_tokens = torch.randint(len(self.tokenizer.vocab), token_ids.shape)\n",
    "        final_tokens = torch.where(\n",
    "            mask_prob & (mask_replace_prob >= 0.8) & (random_replace_prob < 0.5),\n",
    "            random_tokens,\n",
    "            masked_tokens\n",
    "        )\n",
    "        # print(\"10% from masked: \", mask_prob & (mask_replace_prob >= 0.8) & (random_replace_prob < 0.5))\n",
    "        # print(final_tokens)\n",
    "\n",
    "        # Tokens not selected for masking remain unchanged\n",
    "        # final_tokens = torch.where(mask_prob, final_tokens, token_ids)\n",
    "        # print(final_tokens)\n",
    "\n",
    "        # Adding special tokens ids and correcting labels\n",
    "        return self.add_special_tokens(final_tokens, labels)\n",
    "    \n",
    "\n",
    "    def add_special_tokens(self, token_ids, labels):\n",
    "        # Create CLS and PAD tokens\n",
    "        cls_token = torch.tensor([self.tokenizer.vocab[\"[CLS]\"]])\n",
    "        pad_token = torch.tensor([self.tokenizer.vocab[\"[PAD]\"]])\n",
    "\n",
    "        # Add CLS token and truncate or pad token_ids\n",
    "        truncated_tokens = token_ids[:self.max_length]\n",
    "        padded_tokens = torch.cat([cls_token, truncated_tokens, pad_token.repeat(self.max_length - truncated_tokens.size(0) - 1)])\n",
    "\n",
    "        # Add 0 for CLS and PAD tokens to labels\n",
    "        zero_label = torch.tensor([0])\n",
    "        truncated_labels = labels[:self.max_length]\n",
    "        padded_labels = torch.cat([zero_label, truncated_labels, zero_label.repeat(self.max_length - truncated_labels.size(0) - 1)])\n",
    "\n",
    "        # Outputs\n",
    "        final_tokens = padded_tokens  # Shape: (max_length,)\n",
    "        labels = padded_labels         # Shape: (max_length,)\n",
    "        return final_tokens, labels\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        word = self.words[idx]\n",
    "\n",
    "        input_ids, labels = self.mlm_masking_word(word)\n",
    "\n",
    "        attention_mask = torch.where(input_ids != self.tokenizer.vocab[\"[PAD]\"], 1, 0)\n",
    "\n",
    "        output = {\"bert_input\": input_ids,\n",
    "                  \"bert_label\": labels,\n",
    "                  \"attention_mask\": attention_mask}\n",
    "        \n",
    "        return output\n",
    "\n",
    "# Head using embedding layer\n",
    "# If I use the 300 Glove embeddings I can use the embedding layer to predict the word\n",
    "# class MLMHead(torch.nn.Module):\n",
    "#     def __init__(self, embedding_layer):\n",
    "#         \"\"\"\n",
    "#         :param embedding_layer: Embedding layer from the model\n",
    "#         \"\"\"\n",
    "#         super().__init__()\n",
    "#         # Use the embedding layer's weight matrix for the linear layer\n",
    "#         self.linear = torch.nn.Linear(embedding_layer.word_embeddings.weight.size(1),\n",
    "#                                        embedding_layer.word_embeddings.weight.size(0))\n",
    "#         self.linear.weight = embedding_layer.word_embeddings.weight  # Share weights\n",
    "#         self.softmax = torch.nn.LogSoftmax(dim=-1)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return self.softmax(self.linear(x))\n",
    "\n",
    "class MLMHead(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    predicting origin token from masked input sequence\n",
    "    n-class classification problem, n-class = vocab_size\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hidden, vocab_size):\n",
    "        \"\"\"\n",
    "        :param hidden: output size of BERT model\n",
    "        :param vocab_size: total vocab size\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.linear = torch.nn.Linear(hidden, vocab_size)\n",
    "        self.softmax = torch.nn.LogSoftmax(dim=-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.softmax(self.linear(x))\n",
    "\n",
    "# Custom BERT Architecture with Configurable Layers\n",
    "class CustomBERT(Module):\n",
    "    def __init__(self, vocab_size, hidden_size, num_hidden_layers, num_attention_heads, max_position_embeddings, intermediate_size):\n",
    "        super(CustomBERT, self).__init__()\n",
    "        config = BertConfig(\n",
    "            vocab_size=vocab_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_hidden_layers=num_hidden_layers,\n",
    "            num_attention_heads=num_attention_heads,\n",
    "            max_position_embeddings=max_position_embeddings,\n",
    "            intermediate_size=intermediate_size,\n",
    "        )\n",
    "        self.embeddings = BertEmbeddings(config)\n",
    "        self.encoder = BertEncoder(config)\n",
    "        self.mlm_head = MLMHead(hidden_size, vocab_size)\n",
    "        # self.pooler = BertPooler(config)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        embeddings = self.embeddings(input_ids=input_ids)\n",
    "\n",
    "        # NOTE: I have to add to dimension in between for the attention mask\n",
    "        # because it will be used to calculatation the attention scores\n",
    "        attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n",
    "\n",
    "        # Encoder ouputs can return the embeddings in each layer, but here\n",
    "        # I only interested in the last hidden state\n",
    "        encoder_outputs = self.encoder(embeddings, attention_mask=attention_mask, return_dict=True)\n",
    "\n",
    "        # Pooler is used to get the CLS token embedding and apply \n",
    "        # a linear transformation to it + tanh activation\n",
    "        # output = self.pooler(encoder_outputs.last_hidden_state)\n",
    "\n",
    "        # MLM head output\n",
    "        output = self.mlm_head(encoder_outputs.last_hidden_state)\n",
    "\n",
    "        # return encoder_outputs.last_hidden_state, output\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import math\n",
    "\n",
    "\n",
    "class BERTTrainer:\n",
    "    def __init__(\n",
    "        self, \n",
    "        model, \n",
    "        train_dataloader, \n",
    "        test_dataloader=None, \n",
    "        lr= 1e-4,\n",
    "        weight_decay=0.01,\n",
    "        betas=(0.9, 0.999),\n",
    "        log_freq=10,\n",
    "        device='cuda'\n",
    "        ):\n",
    "\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model = model.to(self.device)\n",
    "        self.train_data = train_dataloader\n",
    "        self.test_data = test_dataloader\n",
    "\n",
    "        # Setting the Adam optimizer with hyper-param\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=lr, betas=betas, weight_decay=weight_decay)\n",
    "\n",
    "        # Using Negative Log Likelihood Loss function for predicting the masked_token\n",
    "        self.criterion = torch.nn.NLLLoss(ignore_index=0)\n",
    "        self.log_freq = log_freq\n",
    "        print(\"Total Parameters:\", sum([p.nelement() for p in self.model.parameters()]))\n",
    "    \n",
    "    def train(self, epoch):\n",
    "        self.iteration(epoch, self.train_data)\n",
    "\n",
    "    def test(self, epoch):\n",
    "        self.iteration(epoch, self.test_data, train=False)\n",
    "\n",
    "    def iteration(self, epoch, data_loader, train=True):\n",
    "        \n",
    "        avg_loss = 0.0\n",
    "        total_correct = 0  # To track correct predictions\n",
    "        total_masked = 0   # To track total masked tokens\n",
    "\n",
    "        mode = \"train\" if train else \"test\"\n",
    "\n",
    "        # progress bar\n",
    "        data_iter = tqdm.tqdm(\n",
    "            enumerate(data_loader),\n",
    "            desc=\"EP_%s:%d\" % (mode, epoch),\n",
    "            total=len(data_loader),\n",
    "            bar_format=\"{l_bar}{r_bar}\"\n",
    "        )\n",
    "\n",
    "        for i, data in data_iter:\n",
    "\n",
    "            # batch_data will be sent into the device(GPU or cpu)\n",
    "            data = {key: value.to(self.device) for key, value in data.items()}\n",
    "\n",
    "            # forward the model\n",
    "            mask_lm_output = self.model.forward(data[\"bert_input\"], data[\"attention_mask\"])\n",
    "\n",
    "            # NLLLoss of predicting masked token word\n",
    "            # transpose to (m, vocab_size, seq_len) vs (m, seq_len)\n",
    "            # criterion(mask_lm_output.view(-1, mask_lm_output.size(-1)), data[\"bert_label\"].view(-1))\n",
    "            \n",
    "            # NOTE: the mask_lm_output will return -log probability values,\n",
    "            # then the criterion will only average the values of the masked tokens\n",
    "            loss = self.criterion(mask_lm_output.transpose(1, 2), data[\"bert_label\"])\n",
    "\n",
    "            # backward and optimization only in train\n",
    "            if train:\n",
    "                # self.optim_schedule.zero_grad()\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                # self.optim_schedule.step_and_update_lr()\n",
    "                self.optimizer.step()\n",
    "\n",
    "            # Update average loss\n",
    "            avg_loss += loss.item()\n",
    "\n",
    "            # Calculate predictions and accuracy\n",
    "            predictions = torch.argmax(mask_lm_output, dim=-1)  # Shape: (batch_size, seq_len)\n",
    "            correct = (predictions == data[\"bert_label\"]) & (data[\"bert_label\"] != 0)  # Exclude padding\n",
    "            total_correct += correct.sum().item()\n",
    "            total_masked += (data[\"bert_label\"] != 0).sum().item()  # Exclude padding tokens\n",
    "\n",
    "            # Calculate perplexity\n",
    "            # NOTE: perplexity is not well defined for masked language models like BERT (see summary of the models).\n",
    "            perplexity = math.exp(avg_loss / (i + 1))\n",
    "\n",
    "            # Calculate masked token accuracy\n",
    "            accuracy = total_correct / total_masked if total_masked > 0 else 0\n",
    "\n",
    "            post_fix = {\n",
    "                \"epoch\": epoch,\n",
    "                \"iter\": i,\n",
    "                \"avg_loss\": round(avg_loss / (i + 1), 3),\n",
    "                \"loss\": round(loss.item(), 3),\n",
    "                \"perplexity\": round(perplexity, 3),\n",
    "                \"accuracy\": round(accuracy, 3)\n",
    "            }\n",
    "\n",
    "            if i % self.log_freq == 0:\n",
    "                data_iter.write(str(post_fix))\n",
    "\n",
    "\n",
    "        # Final epoch logging\n",
    "        final_accuracy = total_correct / total_masked if total_masked > 0 else 0\n",
    "        # print(\n",
    "        #     f\"EP{epoch}, {mode}: \\\n",
    "        #     avg_loss={avg_loss / len(data_iter):.4f}, \\\n",
    "        #     perplexity={math.exp(avg_loss / len(data_iter)):.4f}, \\\n",
    "        #     accuracy={final_accuracy:.4f}\"\n",
    "        # )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: ideas para limpiar la data\n",
    "# 1. remover palabras con 1 o 2 caracteres\n",
    "# 2. obtener las raices de las palabras (stemming) -> tokenizer????\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[116], line 64\u001b[0m\n\u001b[1;32m     60\u001b[0m         words\u001b[38;5;241m.\u001b[39mappend(line\u001b[38;5;241m.\u001b[39mstrip())\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m# Generate subwords\u001b[39;00m\n\u001b[0;32m---> 64\u001b[0m subwords \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_subwords\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwords\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_count\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;66;03m# Save subwords to a file\u001b[39;00m\n\u001b[1;32m     67\u001b[0m output_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malgorithmic_subwords.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "Cell \u001b[0;32mIn[116], line 42\u001b[0m, in \u001b[0;36mgenerate_subwords\u001b[0;34m(word_list, min_count, min_length)\u001b[0m\n\u001b[1;32m     40\u001b[0m splits \u001b[38;5;241m=\u001b[39m [word]\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m substring \u001b[38;5;129;01min\u001b[39;00m common_substrings:\n\u001b[0;32m---> 42\u001b[0m     splits \u001b[38;5;241m=\u001b[39m [part \u001b[38;5;28;01mfor\u001b[39;00m split \u001b[38;5;129;01min\u001b[39;00m splits \u001b[38;5;28;01mfor\u001b[39;00m part \u001b[38;5;129;01min\u001b[39;00m split\u001b[38;5;241m.\u001b[39msplit(substring) \u001b[38;5;28;01mif\u001b[39;00m part]\n\u001b[1;32m     43\u001b[0m     subwords\u001b[38;5;241m.\u001b[39mupdate(splits)\n\u001b[1;32m     44\u001b[0m subwords\u001b[38;5;241m.\u001b[39mupdate(splits)\n",
      "Cell \u001b[0;32mIn[116], line 42\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     40\u001b[0m splits \u001b[38;5;241m=\u001b[39m [word]\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m substring \u001b[38;5;129;01min\u001b[39;00m common_substrings:\n\u001b[0;32m---> 42\u001b[0m     splits \u001b[38;5;241m=\u001b[39m [part \u001b[38;5;28;01mfor\u001b[39;00m split \u001b[38;5;129;01min\u001b[39;00m splits \u001b[38;5;28;01mfor\u001b[39;00m part \u001b[38;5;129;01min\u001b[39;00m split\u001b[38;5;241m.\u001b[39msplit(substring) \u001b[38;5;28;01mif\u001b[39;00m part]\n\u001b[1;32m     43\u001b[0m     subwords\u001b[38;5;241m.\u001b[39mupdate(splits)\n\u001b[1;32m     44\u001b[0m subwords\u001b[38;5;241m.\u001b[39mupdate(splits)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "class SuffixTree:\n",
    "    def __init__(self):\n",
    "        self.nodes = {0: {}}  # Root node\n",
    "        self.num = 0  # Node counter\n",
    "\n",
    "    def add_suffix(self, suffix):\n",
    "        current_node = 0\n",
    "        for char in suffix:\n",
    "            if char not in self.nodes[current_node]:\n",
    "                self.num += 1\n",
    "                self.nodes[current_node][char] = self.num\n",
    "                self.nodes[self.num] = {}\n",
    "            current_node = self.nodes[current_node][char]\n",
    "\n",
    "    def find_common_substrings(self, min_count=2, min_length=3):\n",
    "        result = []\n",
    "        stack = [(0, \"\")]\n",
    "        while stack:\n",
    "            node, path = stack.pop()\n",
    "            if len(path) >= min_length and len(self.nodes[node]) >= min_count:\n",
    "                result.append(path)\n",
    "            for char, child_node in self.nodes[node].items():\n",
    "                stack.append((child_node, path + char))\n",
    "        return result\n",
    "\n",
    "\n",
    "def generate_subwords(word_list, min_count=2, min_length=3):\n",
    "    # Build a suffix tree\n",
    "    tree = SuffixTree()\n",
    "    for word in word_list:\n",
    "        for i in range(len(word)):\n",
    "            tree.add_suffix(word[i:])\n",
    "\n",
    "    # Find frequent substrings\n",
    "    common_substrings = tree.find_common_substrings(min_count, min_length)\n",
    "\n",
    "    # Generate subwords by splitting words at common substrings\n",
    "    subwords = set()\n",
    "    for word in word_list:\n",
    "        splits = [word]\n",
    "        for substring in common_substrings:\n",
    "            splits = [part for split in splits for part in split.split(substring) if part]\n",
    "            subwords.update(splits)\n",
    "        subwords.update(splits)\n",
    "\n",
    "    # Filter out very short subwords\n",
    "    subwords = {subword for subword in subwords if len(subword) >= min_length}\n",
    "\n",
    "    return list(subwords)\n",
    "\n",
    "\n",
    "# Example dataset\n",
    "# raw_word_list = [\n",
    "#     \"replaying\", \"submarine\", \"unbelievable\", \"disconnected\", \"transformation\", \"antibacterial\",\n",
    "#     \"automation\", \"endless\", \"happiness\", \"autonomous\", \"misunderstand\"\n",
    "# ]\n",
    "\n",
    "\n",
    "\n",
    "# Generate subwords\n",
    "subwords = generate_subwords(words, min_count=2, min_length=3)\n",
    "\n",
    "# Save subwords to a file\n",
    "output_file = \"algorithmic_subwords.txt\"\n",
    "with open(output_file, \"w\") as f:\n",
    "    for subword in subwords:\n",
    "        f.write(f\"{subword}\\n\")\n",
    "\n",
    "print(f\"Subwords saved to {output_file}:\")\n",
    "print(subwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size:  30\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "hidden_size = 128 # 300 for using Glove embeddings\n",
    "num_hidden_layers = 4  # Change as needed\n",
    "num_attention_heads = 4\n",
    "max_position_embeddings = 32  # Max word length\n",
    "intermediate_size = 512\n",
    "max_word_length = 32\n",
    "batch_size = 16\n",
    "\n",
    "# Initialize Components\n",
    "tokenizer = CharTokenizer()\n",
    "vocab_size = len(tokenizer.vocab)\n",
    "model = CustomBERT(\n",
    "    vocab_size=vocab_size, # 4 special tokens\n",
    "    hidden_size=hidden_size,\n",
    "    num_hidden_layers=num_hidden_layers,\n",
    "    num_attention_heads=num_attention_heads,\n",
    "    max_position_embeddings=max_position_embeddings,\n",
    "    intermediate_size=intermediate_size,\n",
    ")\n",
    "\n",
    "\n",
    "# # Example Dataset\n",
    "# words = [\"apple\", \"banana\", \"cherry\", \"date\"]\n",
    "\n",
    "# Load the dataset in a list from a .txt file\n",
    "words = []\n",
    "with open('words_250000_train.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        words.append(line.strip())\n",
    "\n",
    "train_data = WordDataset(words, tokenizer, max_word_length)\n",
    "train_loader = DataLoader(train_data,\n",
    "                          batch_size=batch_size, \n",
    "                          shuffle=True,\n",
    "                          pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "227300"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Parameters: 805406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:0: 100%|| 1/1 [00:00<00:00, 46.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 0, 'iter': 0, 'avg_loss': 3.663, 'loss': 3.663, 'perplexity': 38.97, 'accuracy': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:1: 100%|| 1/1 [00:00<00:00, 63.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 1, 'iter': 0, 'avg_loss': 3.296, 'loss': 3.296, 'perplexity': 26.999, 'accuracy': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:2: 100%|| 1/1 [00:00<00:00, 61.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 2, 'iter': 0, 'avg_loss': 3.191, 'loss': 3.191, 'perplexity': 24.32, 'accuracy': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:3: 100%|| 1/1 [00:00<00:00, 51.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 3, 'iter': 0, 'avg_loss': 3.578, 'loss': 3.578, 'perplexity': 35.812, 'accuracy': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:4: 100%|| 1/1 [00:00<00:00, 62.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 4, 'iter': 0, 'avg_loss': 3.588, 'loss': 3.588, 'perplexity': 36.177, 'accuracy': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:5: 100%|| 1/1 [00:00<00:00, 58.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 5, 'iter': 0, 'avg_loss': 2.541, 'loss': 2.541, 'perplexity': 12.694, 'accuracy': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:6: 100%|| 1/1 [00:00<00:00, 63.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 6, 'iter': 0, 'avg_loss': 3.241, 'loss': 3.241, 'perplexity': 25.559, 'accuracy': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:7: 100%|| 1/1 [00:00<00:00, 64.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 7, 'iter': 0, 'avg_loss': 3.507, 'loss': 3.507, 'perplexity': 33.335, 'accuracy': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:8: 100%|| 1/1 [00:00<00:00, 63.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 8, 'iter': 0, 'avg_loss': 2.503, 'loss': 2.503, 'perplexity': 12.224, 'accuracy': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:9: 100%|| 1/1 [00:00<00:00, 68.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 9, 'iter': 0, 'avg_loss': 2.882, 'loss': 2.882, 'perplexity': 17.848, 'accuracy': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:10: 100%|| 1/1 [00:00<00:00, 68.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 10, 'iter': 0, 'avg_loss': 3.442, 'loss': 3.442, 'perplexity': 31.238, 'accuracy': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:11: 100%|| 1/1 [00:00<00:00, 66.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 11, 'iter': 0, 'avg_loss': 2.925, 'loss': 2.925, 'perplexity': 18.642, 'accuracy': 0.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:12: 100%|| 1/1 [00:00<00:00, 67.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 12, 'iter': 0, 'avg_loss': 2.99, 'loss': 2.99, 'perplexity': 19.878, 'accuracy': 0.143}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:13: 100%|| 1/1 [00:00<00:00, 68.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 13, 'iter': 0, 'avg_loss': 1.499, 'loss': 1.499, 'perplexity': 4.476, 'accuracy': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:14: 100%|| 1/1 [00:00<00:00, 60.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 14, 'iter': 0, 'avg_loss': nan, 'loss': nan, 'perplexity': nan, 'accuracy': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:15: 100%|| 1/1 [00:00<00:00, 68.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 15, 'iter': 0, 'avg_loss': 3.983, 'loss': 3.983, 'perplexity': 53.67, 'accuracy': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:16: 100%|| 1/1 [00:00<00:00, 72.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 16, 'iter': 0, 'avg_loss': 1.727, 'loss': 1.727, 'perplexity': 5.623, 'accuracy': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:17: 100%|| 1/1 [00:00<00:00, 72.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 17, 'iter': 0, 'avg_loss': 2.716, 'loss': 2.716, 'perplexity': 15.123, 'accuracy': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:18: 100%|| 1/1 [00:00<00:00, 57.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 18, 'iter': 0, 'avg_loss': 2.67, 'loss': 2.67, 'perplexity': 14.447, 'accuracy': 0.333}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:19: 100%|| 1/1 [00:00<00:00, 72.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 19, 'iter': 0, 'avg_loss': 1.594, 'loss': 1.594, 'perplexity': 4.923, 'accuracy': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "bert_trainer = BERTTrainer(model, train_loader, device='cpu')\n",
    "epochs = 20\n",
    "\n",
    "for epoch in range(epochs):\n",
    "  bert_trainer.train(epoch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "character-bert",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
